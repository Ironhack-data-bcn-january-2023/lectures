{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de4729f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# <center> Natural Language Processing (NLP)</center>\n",
    "The [natural language processing](https://es.wikipedia.org/wiki/Procesamiento_de_natural_languages), abbreviated PLN3 —in English, natural language processing, NLP— is a field of sciences of computing, artificial intelligence and linguistics that studies the interactions between computers and human language. It deals with the formulation and investigation of computationally efficient mechanisms for communication between people and machines through natural language, that is, the world's languages. It is not about communication through natural languages ​​in an abstract way, but about designing mechanisms to communicate that are computationally efficient —that can be carried out by means of programs that execute or simulate communication—."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-questionnaire",
   "metadata": {},
   "source": [
    "![elgif](https://media.giphy.com/media/xT0xeJpnrWC4XWblEk/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab13503",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "NLP is considered one of the great challenges of artificial intelligence since it is one of the most complicated and challenging tasks: how to really understand the meaning of a text? How to undertand neologisms, ironies, jokes or poetry? If the strategy/algorithm we use does not overcome these difficulties, the results obtained will be of no use to us.\n",
    "In NLP it is not enough to understand mere words, you must understand the set of words that make up a sentence, and the set of lines that make up a paragraph. Giving a global meaning to the analysis of the text/discourse in order to draw good conclusions.\n",
    "\n",
    "Our language is full of ambiguities, of words with different meanings, twists and different meanings depending on the context. This makes NLP one of the most difficult tasks to master.\n",
    "\n",
    "Therefore, the difficulty of the NLP is at several levels:\n",
    "\n",
    "Ambiguity:\n",
    "\n",
    "- Lexical level: for example, several meanings\n",
    "- Referential level: anaphoras, metaphors, etc...\n",
    "- Structural level: semantics is necessary to understand the structure of a sentence\n",
    "- Pragmatic level: double meanings, irony, humor\n",
    "- Gaps detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-mission",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-data\" data-toc-modified-id=\"The-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The data</a></span></li><li><span><a href=\"#We-bring-all-the-data-to-a-dataframe-from-MySQL\" data-toc-modified-id=\"We-bring-all-the-data-to-a-dataframe-from-MySQL-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>We bring all the data to a dataframe from MySQL</a></span></li><li><span><a href=\"#We-translate\" data-toc-modified-id=\"We-translate-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>We translate</a></span></li><li><span><a href=\"#Sentiment-analysis\" data-toc-modified-id=\"Sentiment-analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sentiment analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#TextBlob\" data-toc-modified-id=\"TextBlob-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>TextBlob</a></span></li><li><span><a href=\"#NLTK\" data-toc-modified-id=\"NLTK-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>NLTK</a></span></li></ul></li><li><span><a href=\"#Adding-to-SQL\" data-toc-modified-id=\"Adding-to-SQL-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Adding to SQL</a></span></li><li><span><a href=\"#Further-processing\" data-toc-modified-id=\"Further-processing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Further processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize:-lemmatization\" data-toc-modified-id=\"Tokenize:-lemmatization-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Tokenize: lemmatization</a></span></li><li><span><a href=\"#Entity-recognition\" data-toc-modified-id=\"Entity-recognition-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Entity recognition</a></span></li></ul></li><li><span><a href=\"#WordClouds\" data-toc-modified-id=\"WordClouds-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>WordClouds</a></span><ul class=\"toc-item\"><li><span><a href=\"#We-generate-a-WordCloud-of-a-song\" data-toc-modified-id=\"We-generate-a-WordCloud-of-a-song-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>We generate a WordCloud of a song</a></span></li><li><span><a href=\"#We-can-also-generate-it-from-a-column-of-an-entire-dataframe\" data-toc-modified-id=\"We-can-also-generate-it-from-a-column-of-an-entire-dataframe-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>We can also generate it from a column of an entire dataframe</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans==4.0.0-rc1\\n\n",
    "#!pip install spacy\n",
    "#!pip install es-core-news-sm\n",
    "#!pip install nltk\n",
    "#!pip install wordcloud\n",
    "#!pip install langdetect\n",
    "#!pip install textblob\n",
    "#python -m spacy download en_core_web_lg\n",
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Databases\n",
    "import sqlalchemy as alch\n",
    "from getpass import getpass\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Languages\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75156fe0",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## The data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9da76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1233a7c",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## We bring all the data to a dataframe from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4be1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0efd5a24",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## We translate\n",
    "A little to our regret, although there are libraries that work in Spanish (the part of Spacy trained in Spanish works very well), the truth is that they work better in English, in general, there are other libraries that are not as exact and even so Spacy works best in English, so let's translate the lyrics.\n",
    "The TextBlob library, which we are going to use later to do sentiment analysis, also translates, but we are better going to use googletrans and its library, be careful when installing it:\n",
    "`pip install googletrans==3.1.0a0`\n",
    "You have to install the alpha version that the official one has issues.\n",
    "We create a column in the dataframe with all the translated letters, and leave the original as well, in case we need it.\n",
    "\n",
    "⚠️ PLEASE INSTALL THE LIBRARY AS IT SAYS ABOVE ⚠️ [stackoverflow](https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed8f5a",
   "metadata": {},
   "source": [
    "`pip install googletrans==4.0.0-rc1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425737fe",
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "# Let's see how to translate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836b049",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Again we continue with the trend of automating and making functions for everything and thus be able to reuse code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b47f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece950f5",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Sentiment analysis\n",
    "### TextBlob\n",
    "`TextBlob(the_string).sentiment`\n",
    "\n",
    "**Arguments:** `string`<br>\n",
    "**Returns:** `polarity`& `subjectivity`\n",
    "\n",
    "\n",
    "The sentiment property returns a named tuple of the form Sentiment(polarity, subjectivity). The polarity score is a float in the range [-1.0, 1.0]. Subjectivity is a float in the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "\n",
    "TextBlob is supported by two libraries, NLTK and pattern, I leave you the [documentation](https://textblob.readthedocs.io/en/dev/)\n",
    "https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54098c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68158f3f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### NLTK\n",
    "The Natural Language Toolkit, or more commonly NLTK, is a set of symbolic and statistical natural language processing libraries and programs for the Python programming language. NLTK includes graphical demonstrations and sample data.\n",
    "\n",
    "In this case we will also get the polarity with the module [SentimentIntensityAnalizer](https://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.vader)\n",
    "\n",
    "`sia.polarity_scores(the_string)`\n",
    "\n",
    "**Aruments:** `string`<br>\n",
    "**Returns:** `polarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ea7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e998ffc",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Information about the [compound](https://github.com/cjhutto/vaderSentiment#about-the-scoring). \n",
    "It is the sum of the scores normalized between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab8427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4fed561",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We check that it works by passing a letter to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6d370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60299a21",
   "metadata": {},
   "source": [
    "## Adding to SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158cb4f",
   "metadata": {},
   "source": [
    "`alter table: new column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb09574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57eb399f",
   "metadata": {},
   "source": [
    "`seed the database: row by row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b956de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-shooting",
   "metadata": {},
   "source": [
    "## Further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9365cb",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Spacy library documentation\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e5150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75deed13",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Tokenize: lemmatization\n",
    "One of the ways to normalize our tokens is through stemming and lemmatization.\n",
    "Stemming consists of removing and replacing suffixes from the root of the word. Lemmatization is a bit more complex and involves doing an analysis of the vocabulary and its morphology to return the basic form of the word (unconjugated, singular, etc).\n",
    "Read [this](https://medium.com/escueladeinteligenciaartificial/procesamiento-de-lenguaje-natural-stemming-y-lemmas-f5efd90dca8) interesting article.\n",
    "When it comes to tokenizing, we are going to do it by previously removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47010a1",
   "metadata": {},
   "source": [
    "![](https://d2mk45aasx86xg.cloudfront.net/difference_between_Stemming_and_lemmatization_8_11zon_452539721d.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577583db",
   "metadata": {},
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb33217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "853741d3",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## WordClouds\n",
    "A word cloud or tag cloud is a visual representation of the words that make up a text, where the size is larger for the words that appear more frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-trance",
   "metadata": {},
   "source": [
    "![wordcloud](https://i.imgur.com/8I8aJ1N.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e799a9e",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### We generate a WordCloud of a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9fdd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "817bc549",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### We can also generate it from a column of an entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-polyester",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-a",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
